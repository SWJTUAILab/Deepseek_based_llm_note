### 1.4强化学习及PPO

#### 1.4.1什么是强化学习？

​	强化学习（Reinforcement Learning, RL），又称再励学习、评价学习或增强学习，是机器学习的范式之一，用于描述和解决智能体（agent）在与环境的交互过程中通过学习策略以达成回报最大化或实现特定目标的问题。强化学习是除了监督学习和无监督学习之外的第三种机器学习范式。

![强化学习](C:\Users\allen\Desktop\deepseek\1-Basic_algorithm\image\强化学习.png)

强化学习系统一般包括四个要素：策略（policy），奖励（reward），价值（value）以及环境或者说是模型（model）。

策略：定义了智能体对于给定状态所做出的行为，是一个从状态到行为的映射。

- 策略定义智能体的行为
- 它是从状态到行为的映射
- 策略本身可以是具体的映射也可以是随机的分布

奖励：定义了强化学习问题的目标，定义智能体表现好坏。

- 奖励是一个标量的反馈信号
- 它能表征在某一步智能体的表现如何
- 智能体的任务就是使得一个时段内积累的总奖励值最大

价值：或者说价值函数，这是强化学习中非常重要的概念，与奖励的即时性不同，价值函数是对长期收益的衡量。

- 价值函数是对未来奖励的预测
- 它可以评估状态的好坏
- 价值函数的计算需要对状态之间的转移进行分析

环境（模型）：是对环境的模拟，但并非所有的强化学习系统都需要有一个模型，因此会有*基于模型*（Model-based）、*不基于模型*（Model-free）两种不同的方法，不基于模型的方法主要是通过对策略和价值函数分析进行学习。

- 模型可以预测环境下一步的表现
- 表现具体可由预测的状态和奖励来反映

#### 1.4.2近端策略优化（PPO）理解

##### 1.传统策略梯度算法

###### 1.1从价值近似到策略近似

强化学习算法可以分为两大类：**基于价值函数**的强化学习和**基于策略**的强化学习。

**基于价值函数的强化学习**通过递归地求解贝尔曼方程来维护Q值函数（可以是离散的列表，也可以是神经网络），每次选择动作时会选择该状态下对应Q值最大的动作，使得未来积累的期望奖励值最大。经典的基于值函数的强化学习算法有Q-Learning、SARSA、DQN算法等。这些算法在学习后的Q值函数不再发生变化，每次做出的策略也是一定的，可以理解为确定性策略。

**基于策略的强化学习**不再通过价值函数来确定选择动作的策略，而是直接学习策略本身，通过一组参数θ对策略进行参数化，并通过神经网络方法优化θ。



###### 1.2定义目标函数

基于参数化策略的思想，我们的目标就是找到那些可能获得更多奖励的动作，使它们对应的概率更大，从而策略就更有可能选择这些动作。



###### 1.3导出梯度策略

与神经网络的优化思路相同，为了通过参数θ优化目标函数，我们需要计算目标函数对θ的导数

对于**离散动作空间**，多使用Softmax策略

对于**连续动作空间**，多使用高斯策略



##### 2.自然梯度策略算法

###### 2.1传统梯度策略算法的缺陷

在传统的策略梯度算法中，我们根据目标函数梯度和步长更新策略权重θ，这样的更新过程可能会出现两个常见的问题：

- **过冲（Overshooting）：**更新错过了奖励峰值并落入了次优策略区域
- **下冲（Undershooting）：**在梯度方向上采取过小的更新步长会导致收敛缓慢

###### 2.2限制策略更新的差异

我们需要表示策略（分布）之间的差异，而不是参数本身的差异。计算两个概率分布之间的差异，最常见的是KL散度，也称为相对熵，描述了两个概率分布之间的距离：

![KL散度](C:\Users\allen\Desktop\deepseek\1-Basic_algorithm\image\KL散度.png)

这样可以确保在参数空间中执行大更新的同时，保证策略本身的改变不超过阈值。



##### 3.信赖域策略优化算法（TRPO）

###### 3.1 自然策略梯度算法的缺陷

- 近似值可能会**违反KL约束**，从而导致分析得出的步长过大，超出限制要求
- 矩阵的**计算时间太长**，是O(N^3)复杂度的运算
- 我们没有检查更新是否真的改进了策略。由于存在大量的近似过程，**策略可能并没有优化**

###### 3.2 算法理论

​	针对自然策略梯度算法的问题，我们希望可以对策略的优化进行量化，从而保证每次的更新一定是优化作用的。为此，需要计算两种策略之间预期回报的差异。采用**原策略预期回报添加新策略预期优势**的方式。

​	TRPO算法采用共轭梯度法来求解上述优化问题。具体步骤包括：

1. **计算优势函数**：利用广义优势估计（GAE）来提高样本的利用率和策略更新的稳定性。
2. **构建目标函数**：基于重要性采样，构造一个在信任区域内的目标函数。
3. **求解优化问题**：通过共轭梯度法求解带约束的优化问题，得到策略参数的更新方向。
4. **线搜索**：在确定的更新方向上进行线搜索，找到合适的步长，使得策略性能得到提升

##### 4.近端策略优化算法（PPO）

###### 4.1TRPO算法缺陷

TRPO仍然存在一些缺点，特别是：

- **无法处理大参数矩阵：**尽管使用了共轭梯度法，TRPO仍然难以处理大的 Fisher矩阵，即使它们不需要求逆
- **二阶优化很慢：**TRPO的实际实现是基于约束的，需要计算上述Fisher矩阵，这大大减慢了更新过程。此外，我们不能利用一阶随机梯度优化器，例如ADAM
- **TRPO 很复杂：**TRPO很难解释、实现和调试。当训练没有产生预期的结果时，确定如何提高性能可能会很麻烦

###### 4.2 PPO Penalty

​	TRPO在理论分析上推导出与KL散度相乘的惩罚项，但在实践中，这种惩罚往往过于严格，只产生非常小的更新。

​	PPO通过设置目标散度 的方式解决了这个问题，希望每次更新都位于目标散度附近的某个地方。目标散度应该大到足以显著改变策略，但又应该小到足以使更新稳定。

###### 4.3PPO Clip

Clipped PPO是目前最流行的PPO的变体，也是我们说PPO时默认的变体。PPO Clip相比于PPO Penalty效果更好，也更容易实现。

与PPO Penalty不同，与其费心随着时间的推移改变惩罚，PPO Clip直接限制策略可以改变的范围。