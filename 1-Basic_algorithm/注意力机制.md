**1.1.注意力机制（Attention）**

1.1.1 传统序列建模的局限性

传统的序列建模方法（如循环神经网络（RNN）、长短时记忆网络（LSTM）等）在处理序列数据时通常采用逐个元素的方式，将序列中先前的信息压缩到固定长度的隐藏状态中。然而，这种方法存在一些局限性：

- **无法有效处理长序列** ：在长序列中，先输入的信息可能会在传播过程中逐渐被遗忘，导致模型难以捕捉到距离较远的上下文信息，这种现象被称为梯度消失问题。
- **无法灵活关注不同位置** ：传统的序列模型在生成每个输出时，只能按照固定顺序逐个处理输入序列，并且对所有输入位置的权重是一样的，无法根据当前任务的需求动态改变对不同位置的关注程度。

例如，在机器翻译任务中，当翻译较长的句子时，传统的 LSTM 模型可能无法很好地记住句子开头的某些重要信息，从而影响翻译质量。
