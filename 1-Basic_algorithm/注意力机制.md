**1.1.注意力机制（Attention）**

1.1.1 传统序列建模的局限性

传统的序列建模方法（如循环神经网络（RNN）、长短时记忆网络（LSTM）等）在处理序列数据时通常采用逐个元素的方式，将序列中先前的信息压缩到固定长度的隐藏状态中。然而，这种方法存在一些局限性：

- **无法有效处理长序列** ：在长序列中，先输入的信息可能会在传播过程中逐渐被遗忘，导致模型难以捕捉到距离较远的上下文信息，这种现象被称为梯度消失问题。
- **无法灵活关注不同位置** ：传统的序列模型在生成每个输出时，只能按照固定顺序逐个处理输入序列，并且对所有输入位置的权重是一样的，无法根据当前任务的需求动态改变对不同位置的关注程度。

例如，在机器翻译任务中，当翻译较长的句子时，传统的 LSTM 模型可能无法很好地记住句子开头的某些重要信息，从而影响翻译质量。


------

1.1.2 注意力机制的基本原理

注意力机制是为了让模型能够动态地关注输入序列中对当前任务最相关的信息部分，从而解决传统序列建模的局限性。其基本思想是，在处理序列中的某个元素时，通过计算输入各个位置与当前元素的相关性，形成一个注意力权重分布。然后根据这个权重分布，对输入序列进行加权求和，得到一个上下文向量，该上下文向量包含了与当前任务最相关的信息。

以机器翻译为例，当翻译英文句子 “The cat is on the mat.” 时，到了翻译 “mat” 这个词时，注意力机制会自动关注到之前输入序列中的 “mat” 一词，以及可能相关的一些上下文信息（如 “on the”），从而帮助更好地翻译出对应的中文词汇。

------
1.1.3 多头注意力机制（Multi-Head Attention）
