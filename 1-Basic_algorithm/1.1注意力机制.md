
#### **1.1.注意力机制（Attention）**

##### 1.1.1 传统序列建模的局限性

传统的序列建模方法（如循环神经网络（RNN）、长短时记忆网络（LSTM）等）在处理序列数据时通常采用逐个元素的方式，将序列中先前的信息压缩到固定长度的隐藏状态中。然而，这种方法存在一些局限性：

- **无法有效处理长序列** ：在长序列中，先输入的信息可能会在传播过程中逐渐被遗忘，导致模型难以捕捉到距离较远的上下文信息，这种现象被称为梯度消失问题。
- **无法灵活关注不同位置** ：传统的序列模型在生成每个输出时，只能按照固定顺序逐个处理输入序列，并且对所有输入位置的权重是一样的，无法根据当前任务的需求动态改变对不同位置的关注程度。

例如，在机器翻译任务中，当翻译较长的句子时，传统的 LSTM 模型可能无法很好地记住句子开头的某些重要信息，从而影响翻译质量。

------

##### 1.1.2 注意力机制的基本原理

注意力机制是为了让模型能够动态地关注输入序列中对当前任务最相关的信息部分，从而解决传统序列建模的局限性。其基本思想是，在处理序列中的某个元素时，通过计算输入各个位置与当前元素的相关性，形成一个注意力权重分布。然后根据这个权重分布，对输入序列进行加权求和，得到一个上下文向量，该上下文向量包含了与当前任务最相关的信息。

以机器翻译为例，当翻译英文句子 “The cat is on the mat.” 时，到了翻译 “mat” 这个词时，注意力机制会自动关注到之前输入序列中的 “mat” 一词，以及可能相关的一些上下文信息（如 “on the”），从而帮助更好地翻译出对应的中文词汇。

------

##### 1.1.3 计算公式与实现细节

attention架构如下图：

![scale_dot_product_attention](https://github.com/SWJTUAILab/Deepseek_based_llm_note/blob/main/1-Basic_algorithm/image/self_attention.jpeg)

通过上述结构图，我们可以知道自注意力的计算步骤如下所示：

对输入 tokens 进行线性变换，即输入序列通过三个线性层分别生成查询（Query）、键（Key）和值（Value）矩阵。

1. 计算注意力得分：查询矩阵与键矩阵的转置相乘，得到注意力得分矩阵。
2. 缩放：除以  `$\sqrt{d_k}$`
3. 应用掩码：根据任务需求应用掩码，将未来位置或填充位置对应值置为 `-inf`。注意，对于训练无需 mask 操作，对于 推理只有 `prefill` 阶段需要 `mask`，用了 kv cache 优化的 `decode` 阶段不需要 `mask` 操作。
4. 归一化：通过 `softmax` 函数将注意力得分归一化为概率分布。
5. 加权求和：将归一化后的注意力得分与值矩阵相乘，得到最终的注意力输出。

------

##### 1.1.4 多头注意力机制（Multi-Head Attention）

代码地址：https://github.com/jadore801120/attention-is-all-you-need-pytorch/blob/fec78a687210851f055f792d45300d27cc60ae41/transformer/SubLayers.py#L9

多头注意力（MHA）是Transformer模型架构中的一个核心组件，它允许模型在处理输入序列时能够同时关注来自不同位置的不同表示子空间的信息。

 

MHA通过将输入向量分割成多个并行的注意力“头”，每个头独立地计算注意力权重并产生输出，然后将这些输出通过拼接和线性变换进行合并以生成最终的注意力表示



![图片](https://github.com/SWJTUAILab/Deepseek_based_llm_note/blob/main/1-Basic_algorithm/image/SDA-MHA.jpg)

多头注意力（MHA）如何进行Q、K、V计算？多头注意力（MHA）通过线性变换将输入张量分别转换为查询（Q）、键（K）和值（V）矩阵，每个矩阵再被分割成多个头进行并行处理。

```
1.输入变换：输入序列首先通过三个不同的线性变换层，分别得到查询（Query）、键（Key）和值（Value）矩阵。这些变换通常是通过全连接层实现的。

2.分头：将查询、键和值矩阵分成多个头（即多个子空间），每个头具有不同的线性变换参数。

 3.注意力计算：对于每个头，都执行一次缩放点积注意力（Scaled Dot-Product Attention）运算。具体来说，计算查询和键的点积，经过缩放、加上偏置后，使用softmax函数得到注意力权重。这些权重用于加权值矩阵，生成加权和作为每个头的输出。

 4.拼接与融合：将所有头的输出拼接在一起，形成一个长向量。然后，对拼接后的向量进行一个最终的线性变换，以整合来自不同头的信息，得到最终的多头注意力输出。
```

验证代码参考/code文件夹中MHA_test.py文件

如何运行：

1.首先确保安装了torch和marplotlib

```bash
pip install torch matplotlib
```

2.代码保存为 MHA_test.py,然后运行：

```bash
python MHA_test.py
```

3.在控制台打印张量形状和可视化注意力权重矩阵

```
Input Q shape: torch.Size([2, 4, 512])
Input K shape: torch.Size([2, 4, 512])
Input V shape: torch.Size([2, 4, 512])
Output shape: torch.Size([2, 4, 512])

Mask matrix:
tensor([[0, 1, 1, 1],
        [0, 0, 1, 1],
        [0, 0, 0, 1],
        [0, 0, 0, 0]], dtype=torch.int32)

Original Attention Scores matrix:
[[[-1.2167113e-02 -5.5046248e-01 -3.7711906e-01  3.9070404e-01]
  [ 3.0970550e-01 -1.7063819e-01 -3.1822443e-01 -4.8890033e-01]
  [-8.8825345e-02  1.7663771e-01  4.9089196e-01 -4.0528470e-01]
  [-2.7068710e-01  5.9133291e-01  9.7594587e-03  2.9229057e-01]]

 [[ 3.8890615e-01  1.2662989e-01 -3.0247056e-01  2.6952615e-01]
  [-2.0101066e-01 -4.6140599e-01  4.0582573e-01 -1.4010453e-01]
  [ 2.4463472e-01 -4.3974945e-01 -6.9146127e-01  1.0820624e-01]
  [-4.5768779e-02 -6.0677689e-01  1.0694212e-01  1.7326725e-01]]

 [[ 8.7778755e-02 -5.3633654e-01  2.1290499e-01 -2.1969864e-01]
  [-3.4260362e-01  7.2139996e-01  1.7059195e-01  4.6677938e-01]
  [-1.4604793e-01 -3.4471750e-01  1.0982702e+00  2.5259212e-01]
  [ 3.5014725e-01 -2.8413305e-01  8.9362357e-03  8.5433853e-01]]

 [[-1.6571021e-01 -7.7321541e-01 -3.7791410e-01 -5.9574199e-01]
  [ 1.2509329e-03  1.8825388e-01  3.4501523e-02 -3.8839275e-01]
  [ 3.3398189e-02  5.6657112e-01  7.1388024e-01  4.2212921e-01]
  [ 1.0490845e-01  3.2609138e-01 -1.9137552e-02 -2.7164474e-02]]

 [[ 3.5540369e-01  2.2404656e-02 -9.8046497e-02 -1.2717080e+00]
  [-1.5099159e-01 -2.4880105e-01 -1.2106145e-01 -5.1991028e-01]
  [ 1.2595375e-01  5.9069467e-01  2.5053588e-01  6.7975640e-02]
  [ 1.4712712e-01 -7.8602940e-01 -1.8135530e-01  3.1453490e-01]]

 [[-5.0739855e-01  4.3263626e-01  8.1785703e-01  5.0831109e-02]
  [ 6.2276959e-02  1.0794875e-01 -5.1444989e-02  2.0414603e-01]
  [ 1.8251736e-01  2.1137886e-01 -1.2557307e-01  2.6895607e-01]
  [ 4.2630720e-01  4.7148934e-01  1.6745725e-01  5.6522721e-01]]

 [[ 3.8470128e-01 -3.0372778e-01 -2.2082676e-01 -3.9761549e-01]
  [-7.5177103e-03 -9.0861648e-02  2.5926059e-01  3.4246236e-01]
  [-6.4983469e-01 -7.5002559e-02  3.7999481e-01  6.3502747e-01]
  [-1.0418557e-01  1.6681254e-01  4.9137205e-01  3.2020557e-01]]

 [[ 3.2134470e-01  9.5628694e-02  3.7501270e-01 -3.3178329e-01]
  [ 4.2416535e-02  3.9302459e-01 -2.0778708e-01 -4.9893698e-01]
  [-1.6394220e-02 -8.6887777e-02 -1.3007745e-01  7.0242714e-03]
  [-1.8672787e-01 -3.6956612e-02  2.4217553e-01  2.4571346e-01]]]

Attention Scores after applying mask:
[[[[-1.21671129e-02            -inf            -inf            -inf]
   [ 3.09705496e-01 -1.70638189e-01            -inf            -inf]
   [-8.88253450e-02  1.76637709e-01  4.90891963e-01            -inf]
   [-2.70687103e-01  5.91332912e-01  9.75945871e-03  2.92290568e-01]]

  [[ 3.88906151e-01            -inf            -inf            -inf]
   [-2.01010659e-01 -4.61405993e-01            -inf            -inf]
   [ 2.44634718e-01 -4.39749449e-01 -6.91461265e-01            -inf]
   [-4.57687788e-02 -6.06776893e-01  1.06942117e-01  1.73267245e-01]]

  [[ 8.77787545e-02            -inf            -inf            -inf]
   [-3.42603624e-01  7.21399963e-01            -inf            -inf]
   [-1.46047935e-01 -3.44717503e-01  1.09827018e+00            -inf]
   [ 3.50147247e-01 -2.84133047e-01  8.93623568e-03  8.54338527e-01]]

  [[-1.65710211e-01            -inf            -inf            -inf]
   [ 1.25093292e-03  1.88253880e-01            -inf            -inf]
   [ 3.33981887e-02  5.66571116e-01  7.13880241e-01            -inf]
   [ 1.04908451e-01  3.26091379e-01 -1.91375520e-02 -2.71644741e-02]]

  [[ 3.55403692e-01            -inf            -inf            -inf]
   [-1.50991589e-01 -2.48801053e-01            -inf            -inf]
   [ 1.25953749e-01  5.90694666e-01  2.50535876e-01            -inf]
   [ 1.47127122e-01 -7.86029398e-01 -1.81355298e-01  3.14534903e-01]]

  [[-5.07398546e-01            -inf            -inf            -inf]
   [ 6.22769594e-02  1.07948750e-01            -inf            -inf]
   [ 1.82517365e-01  2.11378857e-01 -1.25573069e-01            -inf]
   [ 4.26307201e-01  4.71489340e-01  1.67457253e-01  5.65227211e-01]]

  [[ 3.84701282e-01            -inf            -inf            -inf]
   [-7.51771033e-03 -9.08616483e-02            -inf            -inf]
   [-6.49834692e-01 -7.50025585e-02  3.79994810e-01            -inf]
   [-1.04185566e-01  1.66812539e-01  4.91372049e-01  3.20205569e-01]]

  [[ 3.21344703e-01            -inf            -inf            -inf]
   [ 4.24165353e-02  3.93024594e-01            -inf            -inf]
   [-1.63942203e-02 -8.68877769e-02 -1.30077451e-01            -inf]
   [-1.86727867e-01 -3.69566120e-02  2.42175534e-01  2.45713457e-01]]]


 [[[-4.24720287e-01 -1.51887968e-01 -1.20513886e-01  4.72639576e-02]
   [ 2.43559688e-01 -1.74232677e-01 -5.76722138e-02 -2.76623219e-01]
   [-5.32628298e-02  3.33476931e-01 -1.80251524e-02 -3.58454883e-02]
   [ 2.36426443e-01 -8.29851720e-03  6.49924278e-02  4.79957342e-01]]

  [[-7.08811879e-02 -1.78488046e-01 -2.66587645e-01 -6.58081710e-01]
   [-3.49234670e-01 -2.69419789e-01 -1.42776191e-01  2.50712037e-06]
   [-4.95013483e-02  2.37483203e-01 -1.46440610e-01  2.27342322e-01]
   [ 3.23568702e-01 -2.64633060e-01 -3.54428440e-01 -2.80855238e-01]]

  [[-1.06336148e-02  6.22299016e-02  2.75324821e-01 -7.54271626e-01]
   [-3.79212528e-01 -7.47389793e-01  5.82456589e-01 -7.09725395e-02]
   [-2.61828452e-02  3.90586793e-01  1.82775602e-01  1.14719205e-01]
   [-1.42999113e-01  4.27339703e-01  1.62089169e-01 -2.04559967e-01]]

  [[ 4.16329950e-01 -2.32858166e-01  2.38405526e-01  1.55277252e-01]
   [-8.35714519e-01 -4.33994323e-01  1.09975383e-01  1.89407542e-02]
   [-1.18809000e-01 -3.36920619e-01  2.60098934e-01  3.60156178e-01]
   [ 3.68418008e-01  4.39101458e-01  2.27959424e-01 -1.60747945e-01]]

  [[-5.58369495e-02  6.04849517e-01 -4.17775631e-01 -1.49973392e-01]
   [ 3.63577455e-01 -1.85231835e-01 -7.22400099e-02  5.60367286e-01]
   [ 2.82768637e-01 -2.53557980e-01 -8.37606937e-02  2.77176172e-01]
   [ 6.33262217e-01 -8.38438198e-02 -1.80188298e-01 -2.53886491e-01]]

  [[ 3.01787376e-01  3.06002721e-02 -1.21206142e-01  2.69561410e-01]
   [ 3.56435478e-01  1.26270860e-01 -9.15993825e-02  1.41853645e-01]
   [-1.67038754e-01  3.25167254e-02  1.27624407e-01 -3.08401674e-01]
   [-2.76438087e-01 -5.54839194e-01  6.86602414e-01 -5.73135197e-01]]

  [[-6.11580312e-02  3.81740630e-01  1.46142334e-01  1.45895407e-01]
   [-2.18016207e-01  1.33123081e-02 -5.34821630e-01  3.55328470e-02]
   [-9.57430452e-02  5.41796088e-01 -1.63801499e-02 -4.95467335e-01]
   [-3.01846027e-01  6.49061799e-01 -2.31953979e-01  3.94947350e-01]]

  [[ 1.68211862e-01  1.47124901e-01  7.38724172e-01 -3.05113196e-01]
   [ 1.82882875e-01  1.84641153e-01  7.85719216e-01 -4.50123027e-02]
   [-6.65601343e-02  3.59938711e-01  4.05528545e-01 -5.50036132e-01]
   [ 3.42434719e-02  8.14647555e-01  2.12162986e-01 -5.27049422e-01]]]]

Attention weights after softmax:
[[[0.         0.         0.2780615  0.599242  ]
  [0.         0.         0.6028508  0.50826037]
  [0.         0.         0.         1.1111112 ]
  [0.2777778  0.2777778  0.2777778  0.        ]]

 [[0.         0.39615974 0.25793746 0.4570139 ]
  [0.         0.         0.70354545 0.4075657 ]
  [0.         0.         0.         1.1111112 ]
  [0.2777778  0.         0.2777778  0.2777778 ]]

 [[0.         0.         0.523728   0.33980393]
  [0.         0.         0.47387755 0.6372336 ]
  [0.         0.         0.         1.1111112 ]
  [0.         0.2777778  0.2777778  0.2777778 ]]

 [[0.         0.30201218 0.4484372  0.36066183]
  [0.         0.         0.6713063  0.4398049 ]
  [0.         0.         0.         1.1111112 ]
  [0.2777778  0.2777778  0.2777778  0.2777778 ]]

 [[0.         0.51424587 0.45588943 0.14097589]
  [0.         0.         0.6649012  0.44620997]
  [0.         0.         0.         1.1111112 ]
  [0.2777778  0.2777778  0.2777778  0.2777778 ]]

 [[0.         0.3524465  0.5180748  0.24058987]
  [0.         0.         0.48494205 0.6261691 ]
  [0.         0.         0.         1.1111112 ]
  [0.2777778  0.2777778  0.2777778  0.        ]]

 [[0.         0.37076348 0.40281016 0.        ]
  [0.         0.         0.5324573  0.5786538 ]
  [0.         0.         0.         1.1111112 ]
  [0.2777778  0.2777778  0.         0.2777778 ]]

 [[0.         0.37354428 0.4939432  0.24362364]
  [0.         0.         0.635864   0.47524709]
  [0.         0.         0.         1.1111112 ]
  [0.2777778  0.2777778  0.2777778  0.2777778 ]]]

Multi-Head Attention Output:
[[[-0.75727046 -0.37002102  0.09627117 ... -1.8028494   1.0211154
    1.3162816 ]
  [-2.3306713  -0.7874918   0.9401183  ... -0.2974459  -0.08548757
   -0.8157044 ]
  [ 1.4133574  -0.8112461  -0.18861079 ... -0.53949237  1.4594159
   -0.78171575]
  [-0.44487756  1.46367     0.32049334 ...  0.5993342  -2.1886191
    0.00969915]]

 [[-0.7834148  -1.5178694  -1.0630426  ...  0.54844874  1.6111746
   -1.4416035 ]
  [ 0.20243637  0.01241123  0.10187237 ...  0.08329134  0.52670616
   -0.4272001 ]
  [ 1.4083683   0.935148    0.6359573  ... -1.2940316   2.0289063
   -1.1464664 ]
  [-0.4255424  -0.377855   -0.8131723  ... -0.01910849 -1.1049241
    0.24600126]]]

Process finished with exit code 0

```

![7c93b0d57012d6b5d3042e1d48be25d](C:\Users\allen\Documents\WeChat Files\wxid_vb7jfeb1wns412\FileStorage\Temp\7c93b0d57012d6b5d3042e1d48be25d.png)
