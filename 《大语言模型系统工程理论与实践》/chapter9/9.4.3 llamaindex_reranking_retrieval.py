from llama_index.core import VectorStoreIndex, Document
from llama_index.core.retrievers import VectorIndexRetriever
from llama_index.core.query_engine import RetrieverQueryEngine
from llama_index.core.postprocessor import SentenceTransformerRerank
from llama_index.embeddings.huggingface import HuggingFaceEmbedding
from llama_index.llms.ollama import Ollama

# 1. å‡†å¤‡æµ‹è¯•æ–‡æ¡£
documents = [
    Document(text="è‹¹æœæ˜¯ä¸€ç§æ°´æœï¼Œå¯Œå«ç»´ç”Ÿç´ Cå’Œçº¤ç»´ï¼Œæœ‰åŠ©äºæ¶ˆåŒ–ã€‚"),
    Document(text="é¦™è•‰æ˜¯é»„è‰²çš„çƒ­å¸¦æ°´æœï¼Œå‘³é“ç”œç¾ï¼Œå¯Œå«é’¾å…ƒç´ ã€‚"),
    Document(text="è®¡ç®—æœºç§‘å­¦æ˜¯ç ”ç©¶è®¡ç®—æœºç³»ç»Ÿå’Œç®—æ³•çš„å­¦ç§‘ï¼ŒåŒ…æ‹¬æ•°æ®ç»“æ„ã€ç®—æ³•è®¾è®¡ç­‰ã€‚"),
    Document(text="äººå·¥æ™ºèƒ½æ­£åœ¨æ”¹å˜ä¸–ç•Œçš„å„ä¸ªé¢†åŸŸï¼ŒåŒ…æ‹¬åŒ»ç–—ã€æ•™è‚²ã€äº¤é€šç­‰ã€‚"),
    Document(text="æœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œé€šè¿‡æ•°æ®è®­ç»ƒæ¨¡å‹ã€‚"),
    Document(text="æ·±åº¦å­¦ä¹ ä½¿ç”¨ç¥ç»ç½‘ç»œæ¥æ¨¡æ‹Ÿäººè„‘çš„å­¦ä¹ è¿‡ç¨‹ã€‚"),
]

# 2. æ„å»ºå‘é‡ç´¢å¼•
embed_model = HuggingFaceEmbedding(
    model_name="sentence-transformers/all-MiniLM-L6-v2"
)

vector_index = VectorStoreIndex.from_documents(
    documents,
    embed_model=embed_model
)

# 3. åˆ›å»ºæ£€ç´¢å™¨
retriever = VectorIndexRetriever(index=vector_index)

# 4. åˆ›å»ºé‡æ’åºå¤„ç†å™¨
rerank = SentenceTransformerRerank(
    model="cross-encoder/ms-marco-MiniLM-L-12-v2",
    top_n=2
)

# ä½¿ç”¨æœ¬åœ°Ollama LLM
llm = Ollama(
    model="gemma3:1b",
    request_timeout=120.0
)

# 5. åˆ›å»ºæŸ¥è¯¢å¼•æ“
query_engine = RetrieverQueryEngine.from_args(
    retriever=retriever,
    node_postprocessors=[rerank],
    llm=llm
)

# 6. æµ‹è¯•æŸ¥è¯¢
test_queries = [
    "ä»€ä¹ˆæ˜¯æ°´æœï¼Ÿ",
    "ä»‹ç»äººå·¥æ™ºèƒ½æŠ€æœ¯",
    "æœºå™¨å­¦ä¹ çš„åŸºæœ¬æ¦‚å¿µ",
    "è®¡ç®—æœºç§‘å­¦çš„ç ”ç©¶å†…å®¹"
]

print("=== é‡æ’åºæ£€ç´¢æµ‹è¯• ===\n")

for query in test_queries:
    print(f"ğŸ” æŸ¥è¯¢: {query}")
    response = query_engine.query(query)
    print(f"ğŸ“ å›ç­”: {response}")
    print(f"ğŸ“Š æ¥æºèŠ‚ç‚¹æ•°: {len(response.source_nodes)}")
    print("-" * 50)